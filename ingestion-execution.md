# HAI Indexer: Temporal + Redis Streams Architecture

**Document Version:** 1.0  
**Date:** 2026-02-06  
**Author:** Architecture Team  
**Status:** Proposal for Review

---

## Executive Summary

This document proposes a comprehensive architectural upgrade for HAI Indexer, replacing the current RQ-based job queue with **Temporal workflows** and **Redis Streams** for event-driven ingestion. The proposal also introduces **MongoDB as a document registry** to serve as the policy authority for document metadata, ACL management, and lifecycle tracking.

### Key Benefits

- âœ… **70% code reduction** - 8,156 lines â†’ ~1,500 lines (37 orchestrator files â†’ 10 workflows)
- âœ… **Real-time ingestion** - Webhook support for Google Drive, GitHub, S3, etc.
- âœ… **Unlimited job duration** - No more 1-hour timeout limits
- âœ… **Crash recovery** - Automatic resume from last checkpoint
- âœ… **Full observability** - Temporal UI with real-time workflow visibility
- âœ… **Document governance** - MongoDB as single source of truth for ACL policies
- âœ… **Event-driven architecture** - Redis Streams for scalable event processing

### Migration Impact

| Aspect             | Impact Level | Details                                                   |
| ------------------ | ------------ | --------------------------------------------------------- |
| **Code Changes**   | Medium       | ~500 lines modified, ~500 lines added, ~235 lines deleted |
| **Infrastructure** | Medium       | Add Temporal, PostgreSQL, MongoDB to docker-compose       |
| **Migration Time** | 4-6 weeks    | Phased rollout with parallel operation                    |
| **Risk Level**     | Low-Medium   | Can run RQ and Temporal in parallel during migration      |
| **Team Training**  | Medium       | 1-2 weeks to learn Temporal concepts                      |

---

## Table of Contents

1. [Architecture Comparison: Current vs Proposed](#1-architecture-comparison-current-vs-proposed)
2. [How Redis Streams & Temporal Help](#2-how-redis-streams--temporal-help)
3. [Codebase Changes Analysis](#3-codebase-changes-analysis)
4. [Folder Structure: Current vs Refined](#4-folder-structure-current-vs-refined)
5. [MongoDB Document Registry Integration](#5-mongodb-document-registry-integration)
6. [Complete System Architecture](#6-complete-system-architecture)
7. [Migration Roadmap](#7-migration-roadmap)
8. [Operational Considerations](#8-operational-considerations)
9. [Risk Assessment & Mitigation](#9-risk-assessment--mitigation)
10. [Cost-Benefit Analysis](#10-cost-benefit-analysis)
11. [Decision Framework](#11-decision-framework)

---

## 1. Architecture Comparison: Current vs Proposed

### 1.1 Current Architecture (RQ-based)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INGESTION SOURCES (Manual Trigger Only)                                    â”‚
â”‚  âŒ No webhooks, no real-time events                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“ (User clicks "Index Now")
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FASTAPI ROUTES                                                              â”‚
â”‚  POST /api/index/full                                                        â”‚
â”‚  POST /api/index/reindex                                                     â”‚
â”‚                                                                               â”‚
â”‚  Code: app/api/routes_index.py                                               â”‚
â”‚  Lines: 265                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“ queue.enqueue(full_scan_task, job_timeout=3600)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REDIS (RQ Queue)                                                            â”‚
â”‚  - Simple FIFO queue                                                         â”‚
â”‚  - No consumer groups                                                        â”‚
â”‚  - No event replay                                                           â”‚
â”‚  - Jobs deleted after completion                                            â”‚
â”‚                                                                               â”‚
â”‚  redis://redis:6379/0                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“ (RQ Worker polls queue)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RQ WORKER                                                                   â”‚
â”‚  - Single worker process                                                     â”‚
â”‚  - No crash recovery                                                         â”‚
â”‚  - 1 hour timeout limit                                                      â”‚
â”‚  - In-memory state (lost on restart)                                        â”‚
â”‚                                                                               â”‚
â”‚  Code: app/workers/worker.py (60 lines)                                      â”‚
â”‚        app/workers/tasks.py (175 lines)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“ full_scan_task()
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INDEXING PIPELINE                                                           â”‚
â”‚  1. Fetch files from Google Drive                                           â”‚
â”‚  2. Compute hash (dedup check)                                              â”‚
â”‚  3. Fetch permissions (ACL)                                                  â”‚
â”‚  4. Classify document (MEETING, CONTRACT, etc.)                             â”‚
â”‚  5. Chunk document                                                           â”‚
â”‚  6. Generate embeddings (Ollama)                                             â”‚
â”‚  7. Extract meeting metadata (if meeting)                                    â”‚
â”‚  8. Upsert to Qdrant                                                         â”‚
â”‚  9. Build knowledge graph (Neo4j)                                            â”‚
â”‚                                                                               â”‚
â”‚  Code: app/pipeline/indexer.py (800+ lines)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA STORES                                                                 â”‚
â”‚  - Qdrant (vectors + metadata)                                              â”‚
â”‚  - Neo4j (knowledge graph)                                                   â”‚
â”‚  - Redis (dedup hashes)                                                      â”‚
â”‚                                                                               â”‚
â”‚  âŒ No MongoDB (no document registry)                                        â”‚
â”‚  âŒ No centralized ACL management                                            â”‚
â”‚  âŒ No audit trail                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Problems:**

- âŒ Manual trigger only (no webhooks)
- âŒ No crash recovery (in-memory state)
- âŒ 1-hour timeout limit
- âŒ No document registry
- âŒ No observability (logs only)
- âŒ 8,156 lines of custom orchestration code

---

### 1.2 Proposed Architecture (Temporal + Redis Streams + MongoDB)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INGESTION SOURCES (Multi-Channel, Event-Driven)                            â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Google Drive     â”‚  â”‚ GitHub           â”‚  â”‚ S3 / Azure Blob  â”‚          â”‚
â”‚  â”‚ Webhooks         â”‚  â”‚ Webhooks         â”‚  â”‚ Event Notif.     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                     â”‚                     â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ API Uploads      â”‚  â”‚ Email (IMAP)     â”‚  â”‚ MCP Tools        â”‚          â”‚
â”‚  â”‚ Manual Trigger   â”‚  â”‚ Webhooks         â”‚  â”‚ (Tool Calls)     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                     â”‚                     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EVENT ROUTER (FastAPI Webhook Endpoints)                                    â”‚
â”‚                                                                               â”‚
â”‚  POST /api/webhooks/google-drive                                             â”‚
â”‚  POST /api/webhooks/github                                                   â”‚
â”‚  POST /api/webhooks/s3                                                       â”‚
â”‚  POST /api/index/full (manual trigger)                                       â”‚
â”‚  POST /api/index/upload (direct upload)                                      â”‚
â”‚                                                                               â”‚
â”‚  Code: app/api/routes_webhooks.py (NEW)                                      â”‚
â”‚        app/api/routes_index.py (MODIFIED)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“ redis.xadd('file-ingestion-events', {...})
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REDIS STREAMS (Event Bus)                                                   â”‚
â”‚                                                                               â”‚
â”‚  Stream: file-ingestion-events                                               â”‚
â”‚  Consumer Groups: temporal-workers, analytics-workers                        â”‚
â”‚  Retention: 7 days (604,800 seconds)                                         â”‚
â”‚                                                                               â”‚
â”‚  Event Types:                                                                â”‚
â”‚  - file_added (Google Drive, S3, GitHub)                                     â”‚
â”‚  - file_modified (Google Drive, GitHub)                                      â”‚
â”‚  - file_deleted (Google Drive, S3)                                           â”‚
â”‚  - manual_index_request (API)                                                â”‚
â”‚  - batch_upload (API)                                                        â”‚
â”‚                                                                               â”‚
â”‚  redis://redis:6379/0                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“ xreadgroup('temporal-workers', ...)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEMPORAL WORKER (Event Consumer)                                            â”‚
â”‚                                                                               â”‚
â”‚  - Consumes events from Redis Streams                                        â”‚
â”‚  - Starts Temporal workflows for each event                                  â”‚
â”‚  - Acknowledges events after workflow start                                  â”‚
â”‚  - Supports multiple workers (load balancing)                                â”‚
â”‚                                                                               â”‚
â”‚  Code: app/workers/temporal_worker.py (NEW)                                  â”‚
â”‚        app/workers/redis_consumer.py (NEW)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“ temporal_client.start_workflow(...)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEMPORAL WORKFLOWS (Orchestration Layer)                                    â”‚
â”‚                                                                               â”‚
â”‚  FileIngestionWorkflow                                                       â”‚
â”‚  â”œâ”€ fetch_file_metadata                                                      â”‚
â”‚  â”œâ”€ register_in_mongodb                                                      â”‚
â”‚  â”œâ”€ check_acl_policy                                                         â”‚
â”‚  â”œâ”€ download_file                                                            â”‚
â”‚  â”œâ”€ index_file                                                               â”‚
â”‚  â”œâ”€ update_graph                                                             â”‚
â”‚  â””â”€ update_mongodb_after_indexing                                            â”‚
â”‚                                                                               â”‚
â”‚  FullScanWorkflow                                                            â”‚
â”‚  â”œâ”€ list_all_files                                                           â”‚
â”‚  â”œâ”€ register_batch_in_mongodb                                                â”‚
â”‚  â””â”€ Child: FileIngestionWorkflow (for each file)                             â”‚
â”‚                                                                               â”‚
â”‚  Code: app/workflows/ (NEW)                                                  â”‚
â”‚  Temporal Server: localhost:7233                                             â”‚
â”‚  Temporal UI: http://localhost:8080                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“ execute_activity(...)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TEMPORAL ACTIVITIES (Business Logic)                                        â”‚
â”‚                                                                               â”‚
â”‚  File Operations: fetch_file_metadata, download_file, list_all_files        â”‚
â”‚  MongoDB Operations: register_in_mongodb, check_acl_policy, update_mongodb   â”‚
â”‚  Indexing Operations: index_file, update_graph                               â”‚
â”‚                                                                               â”‚
â”‚  Code: app/activities/ (NEW)                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MONGODB DOCUMENT REGISTRY (Policy Authority)                                â”‚
â”‚                                                                               â”‚
â”‚  Collection: documents                                                       â”‚
â”‚  - Document metadata (source, file_name, mime_type, size)                    â”‚
â”‚  - ACL policies (owner, allowed_users, allowed_roles)                        â”‚
â”‚  - Classification (CONFIDENTIAL, PUBLIC, INTERNAL)                           â”‚
â”‚  - Lifecycle status (PENDING, ACTIVE, ARCHIVED, DELETED)                     â”‚
â”‚  - Indexing references (qdrant_point_ids, neo4j_node_id)                     â”‚
â”‚  - Audit trail (created_at, updated_at, indexed_at)                          â”‚
â”‚                                                                               â”‚
â”‚  mongodb://mongodb:27017/hai_indexer                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INDEXING PIPELINE (Enhanced with MongoDB Integration)                       â”‚
â”‚                                                                               â”‚
â”‚  1. âœ… Fetch metadata from MongoDB (not from source)                         â”‚
â”‚  2. âœ… Check ACL policy from MongoDB                                         â”‚
â”‚  3. âœ… Download file (if not cached)                                         â”‚
â”‚  4. âœ… Compute hash (dedup check)                                            â”‚
â”‚  5. âœ… Classify document (update MongoDB)                                    â”‚
â”‚  6. âœ… Chunk document                                                         â”‚
â”‚  7. âœ… Generate embeddings (Ollama)                                          â”‚
â”‚  8. âœ… Extract meeting metadata (if meeting)                                 â”‚
â”‚  9. âœ… Upsert to Qdrant (with MongoDB doc_id reference)                      â”‚
â”‚  10. âœ… Build knowledge graph (Neo4j, with MongoDB doc_id reference)         â”‚
â”‚  11. âœ… Update MongoDB (qdrant_point_ids, neo4j_node_id, indexed_at)        â”‚
â”‚                                                                               â”‚
â”‚  Code: app/pipeline/indexer.py (MODIFIED)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA STORES (Multi-Database Architecture)                                   â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ MongoDB              â”‚  â”‚ Qdrant               â”‚  â”‚ Neo4j            â”‚  â”‚
â”‚  â”‚ (Document Registry)  â”‚  â”‚ (Vector Store)       â”‚  â”‚ (Knowledge Graph)â”‚  â”‚
â”‚  â”‚                      â”‚  â”‚                      â”‚  â”‚                  â”‚  â”‚
â”‚  â”‚ - Metadata           â”‚  â”‚ - Embeddings         â”‚  â”‚ - Entities       â”‚  â”‚
â”‚  â”‚ - ACL policies       â”‚  â”‚ - Chunks             â”‚  â”‚ - Relationships  â”‚  â”‚
â”‚  â”‚ - Classification     â”‚  â”‚ - mongo_doc_id ref   â”‚  â”‚ - mongo_doc_id   â”‚  â”‚
â”‚  â”‚ - Lifecycle          â”‚  â”‚                      â”‚  â”‚                  â”‚  â”‚
â”‚  â”‚ - Audit trail        â”‚  â”‚                      â”‚  â”‚                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚ Redis                â”‚  â”‚ PostgreSQL           â”‚                         â”‚
â”‚  â”‚ (Event Streaming)    â”‚  â”‚ (Temporal State)     â”‚                         â”‚
â”‚  â”‚                      â”‚  â”‚                      â”‚                         â”‚
â”‚  â”‚ - Redis Streams      â”‚  â”‚ - Workflow state     â”‚                         â”‚
â”‚  â”‚ - Dedup hashes       â”‚  â”‚ - Activity history   â”‚                         â”‚
â”‚  â”‚ - Cache (optional)   â”‚  â”‚ - Event sourcing     â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Improvements:**

- âœ… Real-time webhooks (Google Drive, GitHub, S3)
- âœ… Durable state management (PostgreSQL)
- âœ… Unlimited duration (no timeout)
- âœ… MongoDB document registry (policy authority)
- âœ… Full observability (Temporal UI)
- âœ… 70% less code (~1,500 lines vs 8,156 lines)

---

### 1.3 Key Architectural Differences

| Aspect                     | Current (RQ)              | Proposed (Temporal + Redis Streams + MongoDB) |
| -------------------------- | ------------------------- | --------------------------------------------- |
| **Event Sources**          | Manual API only           | Webhooks + API + MCP + Email + Scheduled      |
| **Event Bus**              | RQ (simple queue)         | Redis Streams (event streaming)               |
| **Orchestration**          | RQ Worker (stateless)     | Temporal Workflows (stateful)                 |
| **State Management**       | In-memory (lost on crash) | Durable (PostgreSQL)                          |
| **Document Registry**      | âŒ None                   | âœ… MongoDB (policy authority)                 |
| **ACL Management**         | Qdrant metadata only      | MongoDB (source of truth)                     |
| **Crash Recovery**         | âŒ No                     | âœ… Yes (resume from checkpoint)               |
| **Timeout Limit**          | 1 hour                    | Unlimited (days/weeks)                        |
| **Progress Tracking**      | âŒ No                     | âœ… Yes (Temporal UI)                          |
| **Event Replay**           | âŒ No                     | âœ… Yes (Redis Streams)                        |
| **Consumer Groups**        | âŒ No                     | âœ… Yes (Redis Streams)                        |
| **Observability**          | Logs only                 | Temporal UI + MongoDB audit trail             |
| **Compensation Logic**     | Manual                    | Built-in (Temporal saga pattern)              |
| **Human-in-the-Loop**      | âŒ No                     | âœ… Yes (Temporal signals)                     |
| **Code Complexity**        | 8,156 lines (37 files)    | ~1,500 lines (10 workflows)                   |
| **Retry Logic**            | Custom (manual)           | Built-in (automatic)                          |
| **Multi-Tenant Isolation** | âŒ No                     | âœ… Yes (task queues)                          |

---

## 2. How Redis Streams & Temporal Help

### 2.1 Problems in Current Architecture

#### Problem 1: No Real-Time Ingestion

**Current State:**

```python
# User must manually trigger indexing
POST /api/index/full
â†’ Indexes all files (even unchanged ones)
â†’ Wastes time and resources
```

**Issues:**

- âŒ No Google Drive webhooks â†’ Can't detect file changes in real-time
- âŒ No GitHub webhooks â†’ Can't index new commits automatically
- âŒ No S3 event notifications â†’ Can't index uploaded files automatically

**Impact:**

- ðŸ”´ **Stale data** - Documents indexed hours/days after creation
- ðŸ”´ **Wasted resources** - Re-indexing unchanged files
- ðŸ”´ **Poor UX** - Users must manually trigger indexing

---

#### Problem 2: No Crash Recovery

**Current State:**

```python
# RQ worker crashes after indexing 5,000 files
â†’ All progress lost
â†’ Must restart from beginning
â†’ Re-index all 5,000 files again
```

**Issues:**

- âŒ In-memory state (lost on crash)
- âŒ No checkpointing
- âŒ No resume capability

**Impact:**

- ðŸ”´ **Data loss** - Hours of work lost on crash
- ðŸ”´ **Wasted resources** - Re-processing same files
- ðŸ”´ **Unreliable** - Can't trust long-running jobs

---

#### Problem 3: 1 Hour Timeout Limit

**Current State:**

```python
# RQ job timeout
job = queue.enqueue(full_scan_task, job_timeout=3600)  # 1 hour max
â†’ Indexing 10,000 files takes 2 hours
â†’ Job times out at 1 hour
â†’ Fails with timeout error
```

**Issues:**

- âŒ Hard timeout limit (1 hour)
- âŒ Can't index large document sets
- âŒ No way to extend timeout

**Impact:**

- ðŸ”´ **Can't scale** - Limited to small document sets
- ðŸ”´ **Job failures** - Frequent timeout errors
- ðŸ”´ **Manual intervention** - Must split jobs manually

---

#### Problem 4: No Document Registry

**Current State:**

```python
# Metadata scattered across multiple systems
Qdrant: {file_name, mime_type, tenant_id, ...}
Neo4j: {file_id, source, ...}
Redis: {hash}

# No single source of truth!
# No ACL policy management!
# No lifecycle tracking!
```

**Issues:**

- âŒ No centralized metadata store
- âŒ No ACL policy authority
- âŒ No document lifecycle tracking
- âŒ No audit trail

**Impact:**

- ðŸ”´ **Inconsistent metadata** - Different values in different systems
- ðŸ”´ **No governance** - Can't enforce ACL policies
- ðŸ”´ **No compliance** - Can't track document lifecycle
- ðŸ”´ **No audit trail** - Can't answer "who accessed what when?"

---

#### Problem 5: No Observability

**Current State:**

```python
# Only logs
logger.info(f"Indexing job {job.id} started")
logger.info(f"Indexed 100 files")
logger.error(f"Job failed: {error}")

# No UI, no dashboards, no history
```

**Issues:**

- âŒ No real-time visibility
- âŒ No progress tracking
- âŒ No workflow history
- âŒ No debugging tools

**Impact:**

- ðŸ”´ **Blind execution** - Can't see what's happening
- ðŸ”´ **Hard to debug** - Must grep logs
- ðŸ”´ **No metrics** - Can't measure performance
- ðŸ”´ **No alerts** - Can't detect failures proactively

---

#### Problem 6: Custom Orchestration Code (8,156 Lines)

**Current State:**

```python
# 37 custom orchestrator files
app/orchestrators/workflow_engine.py              (261 lines)
app/orchestrators/export_orchestrator.py          (219 lines)
app/orchestrators/erp_sync_workflow_operator.py   (265 lines)
# ... 34 more files
```

**Issues:**

- âŒ Manual retry logic (reinventing the wheel)
- âŒ Manual state management
- âŒ Manual timeout handling
- âŒ High maintenance burden

**Impact:**

- ðŸ”´ **Technical debt** - 8,156 lines to maintain
- ðŸ”´ **Bug-prone** - Custom retry logic has edge cases
- ðŸ”´ **Hard to extend** - Adding new workflows is complex
- ðŸ”´ **No standardization** - Each orchestrator different

---

### 2.2 How Redis Streams Solves These Problems

#### Solution 1: Real-Time Event Streaming

**Redis Streams enables:**

```python
# Google Drive webhook
POST /api/webhooks/google-drive
{
  "event_type": "file_added",
  "file_id": "1abc...",
  "file_name": "Q4_Report.pdf"
}

# Publish to Redis Streams
redis.xadd('file-ingestion-events', {
    'event_type': 'file_added',
    'source': 'google_drive',
    'file_id': '1abc...',
    'tenant_id': 'tenant_xyz',
    'timestamp': '2026-02-06T10:30:00Z'
})

# Temporal worker consumes event
â†’ Starts FileIngestionWorkflow
â†’ Indexes file in real-time (within seconds!)
```

**Benefits:**

- âœ… **Real-time ingestion** - Files indexed within seconds of creation
- âœ… **Event-driven** - No manual triggers needed
- âœ… **Scalable** - Handles 100K+ events/sec
- âœ… **Durable** - Events persisted (7-day retention)

---

#### Solution 2: Consumer Groups (Parallel Processing)

**Redis Streams consumer groups:**

```python
# Multiple workers consume same stream
Worker 1: xreadgroup('temporal-workers', 'worker-1', ...)
Worker 2: xreadgroup('temporal-workers', 'worker-2', ...)
Worker 3: xreadgroup('temporal-workers', 'worker-3', ...)

# Each worker gets different events (load balancing)
Worker 1 â†’ Processes file_1, file_4, file_7
Worker 2 â†’ Processes file_2, file_5, file_8
Worker 3 â†’ Processes file_3, file_6, file_9

# 3x faster ingestion!
```

**Benefits:**

- âœ… **Parallel processing** - 3x faster ingestion with 3 workers
- âœ… **Load balancing** - Events distributed evenly
- âœ… **Fault tolerance** - If worker crashes, events reassigned
- âœ… **Scalability** - Add more workers to scale horizontally

---

#### Solution 3: Event Replay (Reprocessing)

**Redis Streams event replay:**

```python
# Reprocess events from last 24 hours
events = redis.xread({
    'file-ingestion-events': '1706774400000-0'  # 24 hours ago
})

# Use case: Bug fix deployed, reprocess failed events
for event in events:
    await temporal_client.start_workflow(FileIngestionWorkflow, event)
```

**Benefits:**

- âœ… **Reprocessing** - Replay events after bug fixes
- âœ… **Debugging** - Replay specific events to debug
- âœ… **Disaster recovery** - Rebuild system from event log
- âœ… **Audit trail** - Full event history (7 days)

---

### 2.3 How Temporal Solves These Problems

#### Solution 1: Durable State Management (Crash Recovery)

**Temporal workflow state:**

```python
@workflow.defn
class FullScanWorkflow:
    @workflow.run
    async def run(self, tenant_id: str) -> dict:
        files = await workflow.execute_activity(list_files, tenant_id)
        # âœ… State persisted to PostgreSQL

        indexed = 0
        for file in files:
            await workflow.execute_activity(index_file, file)
            # âœ… Checkpoint after each file
            indexed += 1

        return {"indexed": indexed}

# Server crashes after indexing 5,000 files
# âœ… Temporal resumes from file 5,001 (doesn't re-index first 5,000!)
```

**Benefits:**

- âœ… **Crash recovery** - Resumes from last checkpoint
- âœ… **No data loss** - All progress saved to PostgreSQL
- âœ… **Reliable** - Guaranteed completion
- âœ… **Efficient** - No re-processing

---

#### Solution 2: Unlimited Duration (No Timeout)

**Temporal workflows can run forever:**

```python
@workflow.defn
class FullScanWorkflow:
    @workflow.run
    async def run(self, tenant_id: str) -> dict:
        # âœ… No timeout limit!
        # Can run for days, weeks, months

        files = await workflow.execute_activity(list_files, tenant_id)
        # 100,000 files â†’ 10 hours â†’ NO PROBLEM!

        for file in files:
            await workflow.execute_activity(index_file, file)

        return {"indexed": len(files)}
```

**Benefits:**

- âœ… **No timeout** - Can run for days/weeks
- âœ… **Large datasets** - Index millions of files
- âœ… **No manual splitting** - Single workflow handles everything
- âœ… **Reliable** - Guaranteed completion

---

#### Solution 3: Built-in Retry Logic

**Temporal automatic retries:**

```python
@workflow.defn
class FileIngestionWorkflow:
    @workflow.run
    async def run(self, file_id: str) -> dict:
        # âœ… Automatic retry with exponential backoff
        result = await workflow.execute_activity(
            index_file,
            file_id,
            retry_policy=RetryPolicy(
                initial_interval=timedelta(seconds=1),
                maximum_interval=timedelta(seconds=60),
                backoff_coefficient=2.0,  # Exponential backoff
                maximum_attempts=5,
            )
        )
        return result

# Activity fails â†’ Retry after 1s
# Fails again â†’ Retry after 2s
# Fails again â†’ Retry after 4s
# Fails again â†’ Retry after 8s
# Fails again â†’ Retry after 16s
# Fails again â†’ Workflow fails
```

**Benefits:**

- âœ… **Automatic retries** - No custom code needed
- âœ… **Exponential backoff** - Prevents overwhelming services
- âœ… **Configurable** - Control retry behavior
- âœ… **Reliable** - Handles transient failures

**Replaces 261 lines of custom retry logic in `workflow_engine.py`!**

---

#### Solution 4: Full Observability (Temporal UI)

**Temporal UI provides:**

```
Workflow: FullScanWorkflow
Workflow ID: full-scan-tenant_xyz-20260206
Status: Running
Progress: 5,234 / 10,000 files (52%)
Duration: 1h 23m
Started: 2026-02-06 10:00:00

Steps:
  âœ… list_files (completed in 2.3s)
     Input: {"tenant_id": "tenant_xyz"}
     Output: {"files": [...], "count": 10000}

  âœ… index_file (completed 5,234 times)
     Latest: file_id=1abc... (completed in 1.2s)

  ðŸ”„ index_file (running)
     Input: {"file_id": "1def..."}
     Started: 2026-02-06 11:23:15
```

**Benefits:**

- âœ… **Real-time visibility** - See what's happening now
- âœ… **Progress tracking** - See 52% complete
- âœ… **Full history** - See every step, input, output
- âœ… **Debugging** - Inspect failures, retry history
- âœ… **Search & filter** - Find workflows by status, user, date

---

#### Solution 5: Compensation Logic (Saga Pattern)

**Temporal saga pattern:**

```python
@workflow.defn
class ERPSyncWorkflow:
    @workflow.run
    async def run(self, expenses: list[dict]) -> dict:
        # Step 1: Export to Excel
        excel_file = await workflow.execute_activity(export_to_excel, expenses)

        # Step 2: Push to QuickBooks
        qb_result = await workflow.execute_activity(push_to_quickbooks, excel_file)

        # Step 3: Push to Xero (with compensation)
        try:
            xero_result = await workflow.execute_activity(push_to_xero, excel_file)
        except Exception:
            # âœ… Compensation: Rollback QuickBooks
            await workflow.execute_activity(rollback_quickbooks, qb_result)
            # âœ… Delete Excel file
            await workflow.execute_activity(delete_file, excel_file)
            raise

        return {"qb": qb_result, "xero": xero_result}
```

**Benefits:**

- âœ… **Automatic rollback** - Undo partial changes
- âœ… **Data consistency** - All-or-nothing semantics
- âœ… **Reliable** - Handles distributed transactions
- âœ… **Simple** - Native Python try/except

**Replaces 265 lines of custom compensation logic in `erp_sync_workflow_operator.py`!**

---

### 2.4 Future Benefits

#### Benefit 1: Human-in-the-Loop Workflows

**Temporal signals enable approval workflows:**

```python
@workflow.defn
class DocumentApprovalWorkflow:
    def __init__(self):
        self.approved = False

    @workflow.run
    async def run(self, doc_id: str) -> dict:
        # Step 1: Index document
        result = await workflow.execute_activity(index_document, doc_id)

        # Step 2: Wait for manager approval (can wait days!)
        await workflow.wait_condition(lambda: self.approved, timeout=timedelta(days=7))

        if self.approved:
            # Step 3: Publish to knowledge graph
            await workflow.execute_activity(publish_to_graph, doc_id)
            return {"status": "approved"}
        else:
            # Step 4: Archive document
            await workflow.execute_activity(archive_document, doc_id)
            return {"status": "rejected"}

    @workflow.signal
    def approve(self):
        self.approved = True

# Manager approves via API
POST /api/workflows/{workflow_id}/approve
â†’ Sends signal to workflow
â†’ Workflow resumes and publishes document
```

**Use Cases:**

- âœ… **Compliance review** - Wait for legal approval before publishing
- âœ… **Data quality** - Wait for manual review before indexing
- âœ… **Sensitive documents** - Wait for security clearance
- âœ… **Budget approval** - Wait for manager approval before ERP sync

---

#### Benefit 2: Scheduled Workflows

**Temporal cron schedules:**

```python
# Schedule nightly reindexing
await temporal_client.start_workflow(
    NightlyReindexWorkflow.run,
    id="nightly-reindex",
    task_queue="indexing-tasks",
    cron_schedule="0 2 * * *",  # Every day at 2 AM
)

# Schedule weekly model training
await temporal_client.start_workflow(
    ModelTrainingWorkflow.run,
    id="weekly-model-training",
    task_queue="ml-tasks",
    cron_schedule="0 0 * * 0",  # Every Sunday at midnight
)
```

**Use Cases:**

- âœ… **Nightly reindexing** - Refresh stale documents
- âœ… **Weekly model training** - Retrain embeddings
- âœ… **Monthly analytics** - Generate reports
- âœ… **Quarterly audits** - Compliance checks

---

#### Benefit 3: Multi-Tenant Isolation

**Temporal task queues for tenant isolation:**

```python
# Tenant-specific task queues
await temporal_client.start_workflow(
    FullScanWorkflow.run,
    args=[tenant_id],
    task_queue=f"indexing-{tenant_id}",  # Dedicated queue per tenant
)

# Dedicated workers per tenant
worker_tenant_1 = Worker(
    client=temporal_client,
    task_queue="indexing-tenant_1",
    workflows=[FullScanWorkflow],
)

worker_tenant_2 = Worker(
    client=temporal_client,
    task_queue="indexing-tenant_2",
    workflows=[FullScanWorkflow],
)
```

**Benefits:**

- âœ… **Resource isolation** - Tenant 1 can't starve Tenant 2
- âœ… **Priority queues** - Premium tenants get dedicated workers
- âœ… **Cost allocation** - Track resource usage per tenant
- âœ… **SLA guarantees** - Guarantee response time per tenant

---

## 3. Codebase Changes Analysis

### 3.1 Files to Delete (RQ Components)

```bash
# Delete RQ worker files
app/workers/worker.py                    # 60 lines - DELETE
app/workers/tasks.py                     # 175 lines - DELETE

# Total deleted: 235 lines
```

**Rationale:** RQ is completely replaced by Temporal + Redis Streams.

---

### 3.2 Files to Modify

#### Modify 1: API Routes (`app/api/routes_index.py`)

**BEFORE (RQ-based):**

```python
from rq import Queue

@router.post("/index/full")
async def index_full(queue: Queue = Depends(get_rq_queue)):
    job = queue.enqueue(
        full_scan_task,
        tenant_id,
        access_token,
        admin_id,
        force,
        job_timeout=3600  # 1 hour timeout
    )
    return {"status": "queued", "job_id": job.id}
```

**AFTER (Temporal-based):**

```python
from temporalio.client import Client

@router.post("/index/full")
async def index_full(request: IndexRequest):
    temporal_client = await Client.connect("localhost:7233")

    handle = await temporal_client.start_workflow(
        FullScanWorkflow.run,
        args=[request.tenant_id, settings.google_refresh_token, admin_id, request.force],
        id=f"full-scan-{request.tenant_id}-{uuid4()}",
        task_queue="indexing-tasks",
        # âœ… No timeout limit!
    )

    return {
        "status": "started",
        "workflow_id": handle.id,
        "run_id": handle.result_run_id,
    }
```

**Changes:**

- âŒ Remove `from rq import Queue`
- âŒ Remove `get_rq_queue()` dependency
- âœ… Add `from temporalio.client import Client`
- âœ… Replace `queue.enqueue()` with `temporal_client.start_workflow()`
- âœ… Remove `job_timeout` (no longer needed)

**Lines changed:** ~30 lines

---

#### Modify 2: Indexing Pipeline (`app/pipeline/indexer.py`)

**Add MongoDB Integration:**

```python
class IndexingPipeline:
    def __init__(self, connector, embedding_client, vector_store, mongo_client):
        self.connector = connector
        self.embedding_client = embedding_client
        self.vector_store = vector_store
        self.mongo_client = mongo_client  # âœ… NEW

    async def index_documents(self, tenant_id: str, force: bool = False) -> dict:
        files = await self.connector.list_files()

        for file in files:
            # âœ… NEW: Register in MongoDB first
            doc_id = await self._register_in_mongodb(file, tenant_id)

            # âœ… NEW: Check ACL policy
            allowed = await self._check_acl_policy(doc_id, tenant_id)
            if not allowed:
                continue

            # Index file
            await self._index_single_file(file, tenant_id, doc_id)

            # âœ… NEW: Update MongoDB with indexing results
            await self._update_mongodb_after_indexing(doc_id)

        return {"indexed": len(files)}
```

**Lines changed:** ~100 lines

---

### 3.3 Files to Create (New Components)

#### Create 1: Temporal Workflows

**`app/workflows/file_ingestion.py` (NEW - ~80 lines)**

```python
from temporalio import workflow
from datetime import timedelta

@workflow.defn
class FileIngestionWorkflow:
    """Workflow for ingesting a single file."""

    @workflow.run
    async def run(self, event: dict) -> dict:
        # Step 1: Fetch file metadata
        metadata = await workflow.execute_activity(
            fetch_file_metadata,
            event["file_id"],
            start_to_close_timeout=timedelta(seconds=30),
        )

        # Step 2: Register in MongoDB
        doc_id = await workflow.execute_activity(
            register_in_mongodb,
            metadata,
            start_to_close_timeout=timedelta(seconds=10),
        )

        # Step 3: Check ACL policy
        allowed = await workflow.execute_activity(
            check_acl_policy,
            doc_id,
            event["tenant_id"],
            start_to_close_timeout=timedelta(seconds=10),
        )

        if not allowed:
            return {"status": "denied", "doc_id": doc_id, "indexed": False}

        # Step 4: Download file
        file_content = await workflow.execute_activity(
            download_file,
            event["file_id"],
            start_to_close_timeout=timedelta(minutes=5),
            retry_policy=workflow.RetryPolicy(maximum_attempts=3),
        )

        # Step 5: Index file
        index_result = await workflow.execute_activity(
            index_file,
            file_content,
            doc_id,
            start_to_close_timeout=timedelta(minutes=10),
            retry_policy=workflow.RetryPolicy(maximum_attempts=3),
        )

        # Step 6: Update graph
        await workflow.execute_activity(
            update_graph,
            index_result,
            start_to_close_timeout=timedelta(minutes=5),
        )

        # Step 7: Update MongoDB
        await workflow.execute_activity(
            update_mongodb_after_indexing,
            doc_id,
            index_result,
            start_to_close_timeout=timedelta(seconds=10),
        )

        return {"status": "success", "doc_id": doc_id, "indexed": True}
```

---

**`app/workflows/full_scan.py` (NEW - ~70 lines)**

```python
@workflow.defn
class FullScanWorkflow:
    """Workflow for full scan of all files."""

    @workflow.run
    async def run(self, tenant_id: str, access_token: str, admin_id: str, force: bool = False) -> dict:
        # Step 1: List all files
        files = await workflow.execute_activity(
            list_all_files,
            tenant_id,
            access_token,
            start_to_close_timeout=timedelta(minutes=5),
        )

        # Step 2: Register batch in MongoDB
        await workflow.execute_activity(
            register_batch_in_mongodb,
            files,
            tenant_id,
            start_to_close_timeout=timedelta(minutes=10),
        )

        # Step 3: Index each file (child workflows)
        indexed = 0
        skipped = 0
        failed = 0

        for i, file in enumerate(files):
            # Update progress
            workflow.upsert_search_attributes({
                "progress": i / len(files),
                "indexed": indexed,
            })

            # Start child workflow for each file
            try:
                result = await workflow.execute_child_workflow(
                    FileIngestionWorkflow.run,
                    args=[{
                        "event_type": "manual_index",
                        "source": "google_drive",
                        "file_id": file["id"],
                        "tenant_id": tenant_id,
                    }],
                    id=f"file-ingestion-{file['id']}",
                )

                if result["indexed"]:
                    indexed += 1
                else:
                    skipped += 1

            except Exception as e:
                workflow.logger.error(f"Failed to index file {file['id']}: {e}")
                failed += 1

        return {"indexed": indexed, "skipped": skipped, "failed": failed}
```

---

#### Create 2: Temporal Activities

**`app/activities/file_operations.py` (NEW - ~30 lines)**
**`app/activities/mongodb_operations.py` (NEW - ~100 lines)**
**`app/activities/indexing_operations.py` (NEW - ~40 lines)**

---

#### Create 3: Temporal Worker

**`app/workers/temporal_worker.py` (NEW - ~50 lines)**

```python
import asyncio
from temporalio.client import Client
from temporalio.worker import Worker
from app.workflows.file_ingestion import FileIngestionWorkflow
from app.workflows.full_scan import FullScanWorkflow
from app.activities.file_operations import *
from app.activities.mongodb_operations import *
from app.activities.indexing_operations import *

async def main():
    """Start Temporal worker."""
    client = await Client.connect("localhost:7233")

    worker = Worker(
        client,
        task_queue="indexing-tasks",
        workflows=[FileIngestionWorkflow, FullScanWorkflow],
        activities=[
            fetch_file_metadata,
            download_file,
            list_all_files,
            register_in_mongodb,
            check_acl_policy,
            update_mongodb_after_indexing,
            register_batch_in_mongodb,
            index_file,
            update_graph,
        ],
    )

    await worker.run()

if __name__ == "__main__":
    asyncio.run(main())
```

---

#### Create 4: Redis Streams Consumer

**`app/workers/redis_consumer.py` (NEW - ~70 lines)**

```python
import asyncio
import redis
from temporalio.client import Client
from app.workflows.file_ingestion import FileIngestionWorkflow

async def consume_redis_streams():
    """Consume events from Redis Streams and start Temporal workflows."""
    redis_client = redis.Redis.from_url(settings.redis_url)
    temporal_client = await Client.connect("localhost:7233")

    # Create consumer group
    try:
        redis_client.xgroup_create(
            'file-ingestion-events',
            'temporal-workers',
            id='0',
            mkstream=True
        )
    except redis.ResponseError:
        pass  # Group already exists

    while True:
        try:
            # Read events from stream
            events = redis_client.xreadgroup(
                'temporal-workers',
                'worker-1',
                {'file-ingestion-events': '>'},
                count=10,
                block=1000,
            )

            for stream, messages in events:
                for msg_id, data in messages:
                    event = {key.decode(): value.decode() for key, value in data.items()}

                    # Start Temporal workflow
                    await temporal_client.start_workflow(
                        FileIngestionWorkflow.run,
                        args=[event],
                        id=f"file-ingestion-{event['file_id']}-{msg_id.decode()}",
                        task_queue="indexing-tasks",
                    )

                    # Acknowledge event
                    redis_client.xack('file-ingestion-events', 'temporal-workers', msg_id)

        except Exception as e:
            print(f"Error consuming events: {e}")
            await asyncio.sleep(5)
```

---

#### Create 5: Webhook Routes

**`app/api/routes_webhooks.py` (NEW - ~80 lines)**

```python
from fastapi import APIRouter
from pydantic import BaseModel
import redis

router = APIRouter(prefix="/webhooks", tags=["webhooks"])

@router.post("/google-drive")
async def google_drive_webhook(webhook: GoogleDriveWebhook):
    """Handle Google Drive webhook."""
    redis_client = redis.Redis.from_url(settings.redis_url)

    event_id = redis_client.xadd(
        'file-ingestion-events',
        {
            'event_type': 'file_changed',
            'source': 'google_drive',
            'file_id': webhook.resourceId,
            'channel_id': webhook.channelId,
        }
    )

    return {"status": "queued", "event_id": event_id.decode()}

@router.post("/github")
async def github_webhook(webhook: GitHubWebhook):
    """Handle GitHub webhook."""
    # Similar implementation
    pass

@router.post("/s3")
async def s3_webhook(event: S3Event):
    """Handle S3 event notification."""
    # Similar implementation
    pass
```

---

### 3.4 Summary of Code Changes

| Category   | Action                   | Files        | Lines                           |
| ---------- | ------------------------ | ------------ | ------------------------------- |
| **Delete** | Remove RQ                | 2 files      | -235 lines                      |
| **Modify** | Update API routes        | 3 files      | ~100 lines                      |
| **Modify** | Update indexing pipeline | 1 file       | ~150 lines                      |
| **Create** | Temporal workflows       | 2 files      | +150 lines                      |
| **Create** | Temporal activities      | 3 files      | +170 lines                      |
| **Create** | Temporal worker          | 1 file       | +50 lines                       |
| **Create** | Redis consumer           | 1 file       | +70 lines                       |
| **Create** | Webhook routes           | 1 file       | +80 lines                       |
| **Create** | MongoDB schemas          | 1 file       | +50 lines                       |
| **Create** | Docker compose updates   | 1 file       | +50 lines                       |
| **TOTAL**  |                          | **16 files** | **-235 + 770 = +535 net lines** |

**Key Insight:** Despite adding significant functionality (webhooks, MongoDB, Temporal), the net code increase is only **535 lines** because we're deleting 8,156 lines of custom orchestration code (37 files) and replacing it with ~1,500 lines of Temporal workflows.

---

## 4. Folder Structure: Current vs Refined

### 4.1 Current Folder Structure

**HAI Indexer Current Directory Tree:**

```
hai-indexer/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                          # Application configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ agentic/                           # (1 file)
â”‚   â”‚   â””â”€â”€ answer_generator.py            # Agentic answer generation
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                               # (27 files) - REST API endpoints
â”‚   â”‚   â”œâ”€â”€ main.py                        # FastAPI application entry
â”‚   â”‚   â”œâ”€â”€ routes_index.py                # âŒ Uses RQ - TO BE MODIFIED
â”‚   â”‚   â”œâ”€â”€ routes_settings.py             # âŒ Uses RQ - TO BE MODIFIED
â”‚   â”‚   â”œâ”€â”€ routes_search.py               # Search endpoints
â”‚   â”‚   â”œâ”€â”€ routes_graph.py                # Graph query endpoints
â”‚   â”‚   â”œâ”€â”€ routes_admin.py                # Admin endpoints
â”‚   â”‚   â”œâ”€â”€ routes_feedback.py             # Feedback endpoints
â”‚   â”‚   â”œâ”€â”€ workflow_api.py                # Workflow management API
â”‚   â”‚   â”œâ”€â”€ analytics_api.py               # Analytics endpoints
â”‚   â”‚   â”œâ”€â”€ dashboard_api.py               # Dashboard endpoints
â”‚   â”‚   â””â”€â”€ ... (17 more API files)
â”‚   â”‚
â”‚   â”œâ”€â”€ connectors/                        # (3 files) - External source connectors
â”‚   â”‚   â”œâ”€â”€ base.py                        # Base connector interface
â”‚   â”‚   â””â”€â”€ google_drive.py                # Google Drive connector
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                              # (2 files) - Core utilities
â”‚   â”‚   â””â”€â”€ llm/
â”‚   â”‚       â”œâ”€â”€ client.py                  # LLM client wrapper
â”‚   â”‚       â””â”€â”€ generator.py               # LLM generation utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ domain/                            # (6 files) - Domain logic
â”‚   â”‚   â”œâ”€â”€ document_classifier.py         # Document classification
â”‚   â”‚   â”œâ”€â”€ domain_config_manager.py       # Domain configuration
â”‚   â”‚   â”œâ”€â”€ entity_validator.py            # Entity validation
â”‚   â”‚   â””â”€â”€ spacy_ner_extractor.py         # NER extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/                             # (9 files) - Neo4j knowledge graph
â”‚   â”‚   â”œâ”€â”€ graph_client.py                # Neo4j client
â”‚   â”‚   â”œâ”€â”€ graph_schema.py                # Graph schema definitions
â”‚   â”‚   â””â”€â”€ meeting/                       # Meeting-specific graph logic
â”‚   â”‚       â”œâ”€â”€ entity_resolver.py
â”‚   â”‚       â”œâ”€â”€ prompts.py
â”‚   â”‚       â””â”€â”€ schemas.py
â”‚   â”‚
â”‚   â”œâ”€â”€ metrics/                           # (10 files) - Metrics & monitoring
â”‚   â”‚   â”œâ”€â”€ collector.py                   # Metrics collection
â”‚   â”‚   â”œâ”€â”€ accuracy.py                    # Accuracy metrics
â”‚   â”‚   â”œâ”€â”€ llm_metrics.py                 # LLM performance metrics
â”‚   â”‚   â”œâ”€â”€ graph_quality.py               # Graph quality metrics
â”‚   â”‚   â””â”€â”€ ... (6 more metric files)
â”‚   â”‚
â”‚   â”œâ”€â”€ observability/                     # (11 files) - Observability stack
â”‚   â”‚   â”œâ”€â”€ otel_setup.py                  # OpenTelemetry setup
â”‚   â”‚   â”œâ”€â”€ dashboards/                    # Grafana dashboards
â”‚   â”‚   â”œâ”€â”€ deployment/                    # Deployment configs
â”‚   â”‚   â””â”€â”€ reliability/                   # Reliability monitoring
â”‚   â”‚
â”‚   â”œâ”€â”€ operators/                         # (13 files) - Export & ERP operators
â”‚   â”‚   â”œâ”€â”€ export_base.py                 # Base export operator
â”‚   â”‚   â”œâ”€â”€ csv_export_operator.py         # CSV export
â”‚   â”‚   â”œâ”€â”€ pdf_export_operator.py         # PDF export
â”‚   â”‚   â”œâ”€â”€ excel_export_operator.py       # Excel export
â”‚   â”‚   â”œâ”€â”€ json_export_operator.py        # JSON export
â”‚   â”‚   â”œâ”€â”€ erp_adapter_base.py            # Base ERP adapter
â”‚   â”‚   â”œâ”€â”€ sap_adapter.py                 # SAP integration
â”‚   â”‚   â”œâ”€â”€ oracle_adapter.py              # Oracle integration
â”‚   â”‚   â”œâ”€â”€ quickbooks_adapter.py          # QuickBooks integration
â”‚   â”‚   â”œâ”€â”€ xero_adapter.py                # Xero integration
â”‚   â”‚   â””â”€â”€ tally_adapter.py               # Tally integration
â”‚   â”‚
â”‚   â”œâ”€â”€ orchestrators/                     # âŒ (37 files, 8,156 lines) - TO BE REPLACED
â”‚   â”‚   â”œâ”€â”€ workflow_engine.py             # Custom workflow engine (261 lines)
â”‚   â”‚   â”œâ”€â”€ workflow_models.py             # Workflow data models
â”‚   â”‚   â”œâ”€â”€ workflow_scheduler.py          # Workflow scheduling
â”‚   â”‚   â”œâ”€â”€ workflow_monitoring.py         # Workflow monitoring
â”‚   â”‚   â”œâ”€â”€ workflow_metrics.py            # Workflow metrics
â”‚   â”‚   â”œâ”€â”€ workflow_templates.py          # Workflow templates
â”‚   â”‚   â”œâ”€â”€ export_orchestrator.py         # Export workflows (219 lines)
â”‚   â”‚   â”œâ”€â”€ export_workflow_operator.py    # Export workflow operator
â”‚   â”‚   â”œâ”€â”€ export_models.py               # Export data models
â”‚   â”‚   â”œâ”€â”€ erp_sync_workflow_operator.py  # ERP sync workflows (265 lines)
â”‚   â”‚   â”œâ”€â”€ conditional_workflow_operator.py # Conditional workflows
â”‚   â”‚   â”œâ”€â”€ notification_system.py         # Notification workflows
â”‚   â”‚   â”œâ”€â”€ notification_templates.py      # Notification templates
â”‚   â”‚   â”œâ”€â”€ notification_scheduling.py     # Notification scheduling
â”‚   â”‚   â”œâ”€â”€ notification_preferences.py    # Notification preferences
â”‚   â”‚   â”œâ”€â”€ notification_analytics.py      # Notification analytics
â”‚   â”‚   â”œâ”€â”€ audit_trail.py                 # Audit trail workflows
â”‚   â”‚   â”œâ”€â”€ audit_trail_analytics.py       # Audit analytics
â”‚   â”‚   â”œâ”€â”€ audit_trail_export.py          # Audit export
â”‚   â”‚   â”œâ”€â”€ audit_trail_reports.py         # Audit reports
â”‚   â”‚   â”œâ”€â”€ audit_trail_search.py          # Audit search
â”‚   â”‚   â”œâ”€â”€ dashboard_manager.py           # Dashboard management
â”‚   â”‚   â”œâ”€â”€ dashboard_models.py            # Dashboard models
â”‚   â”‚   â”œâ”€â”€ insights_engine.py             # Insights generation
â”‚   â”‚   â”œâ”€â”€ insights_models.py             # Insights models
â”‚   â”‚   â”œâ”€â”€ intent_detector.py             # Intent detection
â”‚   â”‚   â”œâ”€â”€ performance_optimization.py    # Performance optimization
â”‚   â”‚   â”œâ”€â”€ performance_analytics.py       # Performance analytics
â”‚   â”‚   â”œâ”€â”€ cost_analytics.py              # Cost analytics
â”‚   â”‚   â”œâ”€â”€ trend_analysis.py              # Trend analysis
â”‚   â”‚   â”œâ”€â”€ scalability_ha.py              # Scalability & HA
â”‚   â”‚   â”œâ”€â”€ advanced_testing.py            # Advanced testing
â”‚   â”‚   â”œâ”€â”€ recurring_schedules.py         # Recurring schedules
â”‚   â”‚   â”œâ”€â”€ schedule_optimization.py       # Schedule optimization
â”‚   â”‚   â”œâ”€â”€ schedule_templates.py          # Schedule templates
â”‚   â”‚   â”œâ”€â”€ hai_indexer_integration.py     # HAI Indexer integration
â”‚   â”‚   â””â”€â”€ ... (all custom orchestration logic)
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/                          # (10 files) - Indexing pipeline
â”‚   â”‚   â”œâ”€â”€ indexer.py                     # Main indexing pipeline (800+ lines)
â”‚   â”‚   â”œâ”€â”€ chunker.py                     # Document chunking
â”‚   â”‚   â”œâ”€â”€ embeddings.py                  # Embedding generation
â”‚   â”‚   â”œâ”€â”€ dedup.py                       # Deduplication logic
â”‚   â”‚   â”œâ”€â”€ normalizer.py                  # Data normalization
â”‚   â”‚   â”œâ”€â”€ meeting_metadata_extractor.py  # Meeting metadata extraction
â”‚   â”‚   â”œâ”€â”€ memory_manager.py              # Memory management
â”‚   â”‚   â”œâ”€â”€ performance_monitor.py         # Performance monitoring
â”‚   â”‚   â””â”€â”€ model.py                       # Data models
â”‚   â”‚
â”‚   â”œâ”€â”€ security/                          # (4 files) - Security & ACL
â”‚   â”‚   â”œâ”€â”€ auth.py                        # Authentication
â”‚   â”‚   â”œâ”€â”€ acl.py                         # Access control lists
â”‚   â”‚   â””â”€â”€ audit.py                       # Audit logging
â”‚   â”‚
â”‚   â”œâ”€â”€ structured_output/                 # (4 files) - Structured output
â”‚   â”‚   â”œâ”€â”€ integration.py                 # Integration logic
â”‚   â”‚   â”œâ”€â”€ schemas.py                     # Output schemas
â”‚   â”‚   â””â”€â”€ validator.py                   # Output validation
â”‚   â”‚
â”‚   â”œâ”€â”€ testing/                           # (9 files) - Testing utilities
â”‚   â”‚   â”œâ”€â”€ comparative_tester.py          # Comparative testing
â”‚   â”‚   â”œâ”€â”€ test_case_generator.py         # Test case generation
â”‚   â”‚   â”œâ”€â”€ metrics_calculator.py          # Test metrics
â”‚   â”‚   â””â”€â”€ ... (6 more test files)
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                             # (3 files) - Utilities
â”‚   â”‚   â”œâ”€â”€ entity_utils.py                # Entity utilities
â”‚   â”‚   â””â”€â”€ logging_config.py              # Logging configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ vector/                            # (3 files) - Qdrant vector store
â”‚   â”‚   â”œâ”€â”€ qdrant_client.py               # Qdrant client
â”‚   â”‚   â””â”€â”€ schema.py                      # Vector schema
â”‚   â”‚
â”‚   â””â”€â”€ workers/                           # âŒ (3 files) - RQ workers - TO BE REPLACED
â”‚       â”œâ”€â”€ worker.py                      # RQ worker (60 lines)
â”‚       â””â”€â”€ tasks.py                       # RQ tasks (175 lines)
â”‚
â”œâ”€â”€ docs/                                  # Documentation
â”œâ”€â”€ tests/                                 # Test suite
â”œâ”€â”€ docker-compose.yml                     # Docker services
â”œâ”€â”€ requirements.txt                       # Python dependencies
â””â”€â”€ README.md
```

**Current File Count Summary:**

| Directory            | Files   | Purpose                       | Status in Migration       |
| -------------------- | ------- | ----------------------------- | ------------------------- |
| `api/`               | 27      | REST API endpoints            | âœï¸ Modify (2-3 files)     |
| `orchestrators/`     | 37      | Custom workflow orchestration | âŒ DELETE (8,156 lines)   |
| `operators/`         | 13      | Export & ERP operators        | âœ… Keep (unchanged)       |
| `observability/`     | 11      | Observability stack           | âœ… Keep (unchanged)       |
| `pipeline/`          | 10      | Indexing pipeline             | âœï¸ Modify (1 file)        |
| `metrics/`           | 10      | Metrics & monitoring          | âœ… Keep (unchanged)       |
| `testing/`           | 9       | Testing utilities             | âœ… Keep (unchanged)       |
| `graph/`             | 9       | Neo4j knowledge graph         | âœ… Keep (unchanged)       |
| `domain/`            | 6       | Domain logic                  | âœ… Keep (unchanged)       |
| `security/`          | 4       | Security & ACL                | âœ… Keep (unchanged)       |
| `structured_output/` | 4       | Structured output             | âœ… Keep (unchanged)       |
| `workers/`           | 3       | RQ workers                    | âŒ DELETE (235 lines)     |
| `connectors/`        | 3       | External source connectors    | âœ… Keep (unchanged)       |
| `vector/`            | 3       | Qdrant vector store           | âœ… Keep (unchanged)       |
| `utils/`             | 3       | Utilities                     | âœ… Keep (unchanged)       |
| `core/`              | 2       | Core utilities                | âœ… Keep (unchanged)       |
| `agentic/`           | 1       | Agentic answer generation     | âœ… Keep (unchanged)       |
| **TOTAL**            | **155** | **Total Python files**        | **Delete 40, Modify 3-4** |

---

### 4.2 Proposed Refined Folder Structure

**HAI Indexer Refined Directory Tree (with Temporal + Redis Streams + MongoDB):**

```
hai-indexer/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                          # Application configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ agentic/                           # (1 file) - UNCHANGED
â”‚   â”‚   â””â”€â”€ answer_generator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                               # (30 files) - âœï¸ MODIFIED + NEW
â”‚   â”‚   â”œâ”€â”€ main.py                        # FastAPI application entry
â”‚   â”‚   â”œâ”€â”€ routes_index.py                # âœï¸ MODIFIED - Uses Temporal instead of RQ
â”‚   â”‚   â”œâ”€â”€ routes_settings.py             # âœï¸ MODIFIED - Uses Temporal instead of RQ
â”‚   â”‚   â”œâ”€â”€ routes_search.py               # UNCHANGED
â”‚   â”‚   â”œâ”€â”€ routes_graph.py                # UNCHANGED
â”‚   â”‚   â”œâ”€â”€ routes_admin.py                # UNCHANGED
â”‚   â”‚   â”œâ”€â”€ routes_webhooks.py             # âœ… NEW - Webhook endpoints
â”‚   â”‚   â”œâ”€â”€ routes_workflows.py            # âœ… NEW - Temporal workflow management
â”‚   â”‚   â”œâ”€â”€ routes_registry.py             # âœ… NEW - MongoDB document registry API
â”‚   â”‚   â””â”€â”€ ... (24 more API files)
â”‚   â”‚
â”‚   â”œâ”€â”€ workflows/                         # âœ… NEW (10 files) - Temporal workflows
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ file_ingestion.py              # âœ… NEW - File ingestion workflow
â”‚   â”‚   â”œâ”€â”€ full_scan.py                   # âœ… NEW - Full scan workflow
â”‚   â”‚   â”œâ”€â”€ erp_sync.py                    # âœ… NEW - ERP sync workflow
â”‚   â”‚   â”œâ”€â”€ export.py                      # âœ… NEW - Export workflow
â”‚   â”‚   â”œâ”€â”€ document_approval.py           # âœ… NEW - Document approval workflow
â”‚   â”‚   â”œâ”€â”€ notification.py                # âœ… NEW - Notification workflow
â”‚   â”‚   â”œâ”€â”€ audit_trail.py                 # âœ… NEW - Audit trail workflow
â”‚   â”‚   â”œâ”€â”€ analytics.py                   # âœ… NEW - Analytics workflow
â”‚   â”‚   â”œâ”€â”€ lifecycle_management.py        # âœ… NEW - Document lifecycle workflow
â”‚   â”‚   â””â”€â”€ common.py                      # âœ… NEW - Common workflow utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ activities/                        # âœ… NEW (8 files) - Temporal activities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ file_operations.py             # âœ… NEW - File fetch/download/list
â”‚   â”‚   â”œâ”€â”€ mongodb_operations.py          # âœ… NEW - MongoDB CRUD operations
â”‚   â”‚   â”œâ”€â”€ indexing_operations.py         # âœ… NEW - Indexing activities
â”‚   â”‚   â”œâ”€â”€ graph_operations.py            # âœ… NEW - Graph update activities
â”‚   â”‚   â”œâ”€â”€ notification_operations.py     # âœ… NEW - Notification activities
â”‚   â”‚   â”œâ”€â”€ export_operations.py           # âœ… NEW - Export activities
â”‚   â”‚   â”œâ”€â”€ erp_operations.py              # âœ… NEW - ERP sync activities
â”‚   â”‚   â””â”€â”€ analytics_operations.py        # âœ… NEW - Analytics activities
â”‚   â”‚
â”‚   â”œâ”€â”€ workers/                           # âœï¸ REPLACED (2 files) - Temporal workers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ temporal_worker.py             # âœ… NEW - Temporal worker (replaces RQ worker)
â”‚   â”‚   â””â”€â”€ redis_consumer.py              # âœ… NEW - Redis Streams consumer
â”‚   â”‚
â”‚   â”œâ”€â”€ events/                            # âœ… NEW (5 files) - Event schemas & publishers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ schemas.py                     # âœ… NEW - Event schemas (Pydantic models)
â”‚   â”‚   â”œâ”€â”€ publishers.py                  # âœ… NEW - Redis Streams publishers
â”‚   â”‚   â”œâ”€â”€ consumers.py                   # âœ… NEW - Redis Streams consumer utilities
â”‚   â”‚   â””â”€â”€ handlers.py                    # âœ… NEW - Event handlers
â”‚   â”‚
â”‚   â”œâ”€â”€ registry/                          # âœ… NEW (6 files) - MongoDB document registry
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ client.py                      # âœ… NEW - MongoDB client wrapper
â”‚   â”‚   â”œâ”€â”€ schemas.py                     # âœ… NEW - Document registry schemas
â”‚   â”‚   â”œâ”€â”€ acl_manager.py                 # âœ… NEW - ACL policy management
â”‚   â”‚   â”œâ”€â”€ lifecycle_manager.py           # âœ… NEW - Document lifecycle management
â”‚   â”‚   â””â”€â”€ audit_logger.py                # âœ… NEW - Audit trail logging
â”‚   â”‚
â”‚   â”œâ”€â”€ connectors/                        # (5 files) - âœï¸ MODIFIED + NEW
â”‚   â”‚   â”œâ”€â”€ base.py                        # UNCHANGED
â”‚   â”‚   â”œâ”€â”€ google_drive.py                # âœï¸ MODIFIED - Add webhook support
â”‚   â”‚   â”œâ”€â”€ github.py                      # âœ… NEW - GitHub connector
â”‚   â”‚   â”œâ”€â”€ s3.py                          # âœ… NEW - S3 connector
â”‚   â”‚   â””â”€â”€ email.py                       # âœ… NEW - Email connector
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                              # (2 files) - UNCHANGED
â”‚   â”‚   â””â”€â”€ llm/
â”‚   â”‚       â”œâ”€â”€ client.py
â”‚   â”‚       â””â”€â”€ generator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ domain/                            # (6 files) - UNCHANGED
â”‚   â”‚   â”œâ”€â”€ document_classifier.py
â”‚   â”‚   â”œâ”€â”€ domain_config_manager.py
â”‚   â”‚   â”œâ”€â”€ entity_validator.py
â”‚   â”‚   â””â”€â”€ spacy_ner_extractor.py
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/                             # (9 files) - UNCHANGED
â”‚   â”‚   â”œâ”€â”€ graph_client.py
â”‚   â”‚   â”œâ”€â”€ graph_schema.py
â”‚   â”‚   â””â”€â”€ meeting/
â”‚   â”‚       â”œâ”€â”€ entity_resolver.py
â”‚   â”‚       â”œâ”€â”€ prompts.py
â”‚   â”‚       â””â”€â”€ schemas.py
â”‚   â”‚
â”‚   â”œâ”€â”€ metrics/                           # (10 files) - UNCHANGED
â”‚   â”‚   â”œâ”€â”€ collector.py
â”‚   â”‚   â”œâ”€â”€ accuracy.py
â”‚   â”‚   â”œâ”€â”€ llm_metrics.py
â”‚   â”‚   â””â”€â”€ ... (7 more files)
â”‚   â”‚
â”‚   â”œâ”€â”€ observability/                     # (11 files) - UNCHANGED
â”‚   â”‚   â”œâ”€â”€ otel_setup.py
â”‚   â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â”œâ”€â”€ deployment/
â”‚   â”‚   â””â”€â”€ reliability/
â”‚   â”‚
â”‚   â”œâ”€â”€ operators/                         # (13 files) - UNCHANGED
â”‚   â”‚   â”œâ”€â”€ export_base.py
â”‚   â”‚   â”œâ”€â”€ csv_export_operator.py
â”‚   â”‚   â”œâ”€â”€ pdf_export_operator.py
â”‚   â”‚   â”œâ”€â”€ erp_adapter_base.py
â”‚   â”‚   â””â”€â”€ ... (9 more files)
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/                          # (10 files) - âœï¸ MODIFIED
â”‚   â”‚   â”œâ”€â”€ indexer.py                     # âœï¸ MODIFIED - Add MongoDB integration
â”‚   â”‚   â”œâ”€â”€ chunker.py                     # UNCHANGED
â”‚   â”‚   â”œâ”€â”€ embeddings.py                  # UNCHANGED
â”‚   â”‚   â”œâ”€â”€ dedup.py                       # UNCHANGED
â”‚   â”‚   â””â”€â”€ ... (6 more files)
â”‚   â”‚
â”‚   â”œâ”€â”€ security/                          # (4 files) - UNCHANGED
â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”œâ”€â”€ acl.py
â”‚   â”‚   â””â”€â”€ audit.py
â”‚   â”‚
â”‚   â”œâ”€â”€ structured_output/                 # (4 files) - UNCHANGED
â”‚   â”‚   â”œâ”€â”€ integration.py
â”‚   â”‚   â”œâ”€â”€ schemas.py
â”‚   â”‚   â””â”€â”€ validator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ testing/                           # (9 files) - UNCHANGED
â”‚   â”‚   â”œâ”€â”€ comparative_tester.py
â”‚   â”‚   â”œâ”€â”€ test_case_generator.py
â”‚   â”‚   â””â”€â”€ ... (7 more files)
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                             # (3 files) - UNCHANGED
â”‚   â”‚   â”œâ”€â”€ entity_utils.py
â”‚   â”‚   â””â”€â”€ logging_config.py
â”‚   â”‚
â”‚   â””â”€â”€ vector/                            # (3 files) - UNCHANGED
â”‚       â”œâ”€â”€ qdrant_client.py
â”‚       â””â”€â”€ schema.py
â”‚
â”œâ”€â”€ docs/                                  # Documentation
â”‚   â””â”€â”€ TEMPORAL_REDIS_STREAMS_ARCHITECTURE.md  # âœ… NEW - This document
â”‚
â”œâ”€â”€ tests/                                 # Test suite
â”‚   â”œâ”€â”€ test_workflows/                    # âœ… NEW - Workflow tests
â”‚   â”œâ”€â”€ test_activities/                   # âœ… NEW - Activity tests
â”‚   â””â”€â”€ test_registry/                     # âœ… NEW - Registry tests
â”‚
â”œâ”€â”€ docker-compose.yml                     # âœï¸ MODIFIED - Add Temporal, MongoDB, PostgreSQL
â”œâ”€â”€ requirements.txt                       # âœï¸ MODIFIED - Add temporalio, motor (MongoDB)
â””â”€â”€ README.md
```

**Refined File Count Summary:**

| Directory            | Files   | Change from Current | Purpose                                     |
| -------------------- | ------- | ------------------- | ------------------------------------------- |
| `workflows/`         | 10      | âœ… **+10 NEW**      | Temporal workflows (replaces orchestrators) |
| `activities/`        | 8       | âœ… **+8 NEW**       | Temporal activities                         |
| `events/`            | 5       | âœ… **+5 NEW**       | Event schemas & Redis Streams               |
| `registry/`          | 6       | âœ… **+6 NEW**       | MongoDB document registry                   |
| `api/`               | 30      | âœï¸ +3 files         | REST API endpoints (3 new routes)           |
| `connectors/`        | 5       | âœï¸ +2 files         | Source connectors (GitHub, S3, Email)       |
| `workers/`           | 2       | âœï¸ Replaced         | Temporal workers (replaces RQ)              |
| `pipeline/`          | 10      | âœï¸ Modified         | Indexing pipeline (MongoDB integration)     |
| `orchestrators/`     | 0       | âŒ **-37 DELETED**  | Custom orchestration (replaced by Temporal) |
| `operators/`         | 13      | âœ… Unchanged        | Export & ERP operators                      |
| `observability/`     | 11      | âœ… Unchanged        | Observability stack                         |
| `metrics/`           | 10      | âœ… Unchanged        | Metrics & monitoring                        |
| `testing/`           | 9       | âœ… Unchanged        | Testing utilities                           |
| `graph/`             | 9       | âœ… Unchanged        | Neo4j knowledge graph                       |
| `domain/`            | 6       | âœ… Unchanged        | Domain logic                                |
| `security/`          | 4       | âœ… Unchanged        | Security & ACL                              |
| `structured_output/` | 4       | âœ… Unchanged        | Structured output                           |
| `vector/`            | 3       | âœ… Unchanged        | Qdrant vector store                         |
| `utils/`             | 3       | âœ… Unchanged        | Utilities                                   |
| `core/`              | 2       | âœ… Unchanged        | Core utilities                              |
| `agentic/`           | 1       | âœ… Unchanged        | Agentic answer generation                   |
| **TOTAL**            | **151** | **-4 net files**    | **155 â†’ 151 files**                         |

---

### 4.3 Key Differences: Current vs Refined

| Aspect                 | Current Structure                        | Refined Structure                                  |
| ---------------------- | ---------------------------------------- | -------------------------------------------------- |
| **Workflow Logic**     | `orchestrators/` (37 files, 8,156 lines) | `workflows/` (10 files, ~1,500 lines)              |
| **Background Jobs**    | `workers/` (RQ-based, 3 files)           | `workers/` (Temporal-based, 2 files)               |
| **Event Handling**     | âŒ No event system                       | âœ… `events/` (5 files) - Redis Streams             |
| **Document Registry**  | âŒ No centralized registry               | âœ… `registry/` (6 files) - MongoDB                 |
| **Activity Logic**     | âŒ Mixed with orchestrators              | âœ… `activities/` (8 files) - Separated concerns    |
| **Webhooks**           | âŒ No webhook support                    | âœ… `api/routes_webhooks.py` - Full webhook support |
| **Source Connectors**  | Google Drive only (3 files)              | Google Drive, GitHub, S3, Email (5 files)          |
| **ACL Management**     | `security/acl.py` (scattered logic)      | `registry/acl_manager.py` (centralized)            |
| **Lifecycle Tracking** | âŒ No lifecycle management               | âœ… `registry/lifecycle_manager.py`                 |
| **Audit Trail**        | `security/audit.py` (basic logging)      | `registry/audit_logger.py` (comprehensive)         |
| **Total Files**        | 155 files                                | 151 files (-4 net)                                 |
| **Total Lines (est.)** | ~25,000 lines                            | ~18,000 lines (-28% reduction)                     |

---

### 4.4 File Organization Principles

#### **Separation of Concerns**

**Current Problem:**

- Workflow logic mixed with business logic in `orchestrators/`
- No clear separation between orchestration and execution

**Refined Solution:**

- **`workflows/`** - Pure orchestration logic (what to do, when to do it)
- **`activities/`** - Pure business logic (how to do it)
- **Clear boundaries** - Workflows call activities, activities don't know about workflows

**Example:**

```python
# workflows/file_ingestion.py (WHAT to do)
@workflow.defn
class FileIngestionWorkflow:
    async def run(self, event: dict):
        # Orchestration: define the steps
        metadata = await workflow.execute_activity(fetch_file_metadata, ...)
        doc_id = await workflow.execute_activity(register_in_mongodb, ...)
        result = await workflow.execute_activity(index_file, ...)
        return result

# activities/file_operations.py (HOW to do it)
@activity.defn
async def fetch_file_metadata(file_id: str):
    # Business logic: actual implementation
    connector = GoogleDriveConnector()
    return await connector.get_file_metadata(file_id)
```

---

#### **Event-Driven Patterns**

**Current Problem:**

- No event system
- Manual API triggers only
- No real-time ingestion

**Refined Solution:**

- **`events/`** directory as first-class citizen
- **Event schemas** - Pydantic models for type safety
- **Event publishers** - Redis Streams integration
- **Event consumers** - Temporal workflow triggers

**Example:**

```python
# events/schemas.py
class FileChangedEvent(BaseModel):
    event_type: str = "file_changed"
    source: str  # google_drive, github, s3
    file_id: str
    tenant_id: str
    timestamp: datetime

# events/publishers.py
async def publish_file_changed_event(event: FileChangedEvent):
    redis_client.xadd('file-ingestion-events', event.dict())

# workers/redis_consumer.py
async def consume_events():
    for event in redis_client.xreadgroup(...):
        await temporal_client.start_workflow(FileIngestionWorkflow, event)
```

---

#### **Document Registry as First-Class Citizen**

**Current Problem:**

- Metadata scattered across Qdrant, Neo4j, Redis
- No single source of truth
- No ACL policy management

**Refined Solution:**

- **`registry/`** directory dedicated to document management
- **MongoDB as policy authority** - All ACL decisions go through registry
- **Lifecycle management** - Track document state transitions
- **Audit trail** - Comprehensive access logging

**Example:**

```python
# registry/acl_manager.py
class ACLManager:
    async def check_access(self, doc_id: str, user: str) -> bool:
        # Single source of truth for ACL
        doc = await self.mongo_client.documents.find_one({"_id": doc_id})
        return self._evaluate_acl(doc["acl"], user)

# registry/lifecycle_manager.py
class LifecycleManager:
    async def transition(self, doc_id: str, new_status: str):
        # Track lifecycle: PENDING â†’ ACTIVE â†’ ARCHIVED â†’ DELETED
        await self.mongo_client.documents.update_one(
            {"_id": doc_id},
            {"$set": {"lifecycle_status": new_status}}
        )
```

---

#### **Temporal-Specific Organization**

**Current Problem:**

- Custom workflow engine with manual retry logic
- State management scattered across files
- No clear workflow boundaries

**Refined Solution:**

- **`workflows/`** - Durable workflows with automatic state management
- **`activities/`** - Retriable units of work with timeout policies
- **`workers/`** - Temporal workers that execute workflows and activities

**Benefits:**

- âœ… **Automatic state persistence** - Temporal handles checkpointing
- âœ… **Built-in retry logic** - No manual retry code needed
- âœ… **Timeout management** - Declarative timeout policies
- âœ… **Observability** - Temporal UI shows all workflow state

---

### 4.5 Migration Impact on Folder Structure

**Files to Delete:**

```
âŒ app/orchestrators/                      (37 files, 8,156 lines)
âŒ app/workers/worker.py                   (60 lines)
âŒ app/workers/tasks.py                    (175 lines)
```

**Total Deletion:** 37 + 2 = **39 files, 8,391 lines**

---

**Files to Create:**

```
âœ… app/workflows/                          (10 files, ~1,500 lines)
âœ… app/activities/                         (8 files, ~800 lines)
âœ… app/events/                             (5 files, ~300 lines)
âœ… app/registry/                           (6 files, ~600 lines)
âœ… app/workers/temporal_worker.py          (~50 lines)
âœ… app/workers/redis_consumer.py           (~70 lines)
âœ… app/api/routes_webhooks.py              (~80 lines)
âœ… app/api/routes_workflows.py             (~100 lines)
âœ… app/api/routes_registry.py              (~120 lines)
âœ… app/connectors/github.py                (~150 lines)
âœ… app/connectors/s3.py                    (~100 lines)
âœ… app/connectors/email.py                 (~120 lines)
```

**Total Creation:** 32 + 3 = **35 files, ~3,990 lines**

---

**Files to Modify:**

```
âœï¸ app/api/routes_index.py                (~30 lines changed)
âœï¸ app/api/routes_settings.py             (~20 lines changed)
âœï¸ app/pipeline/indexer.py                (~100 lines changed)
âœï¸ app/connectors/google_drive.py         (~50 lines changed)
âœï¸ docker-compose.yml                     (~50 lines added)
âœï¸ requirements.txt                       (~5 lines added)
```

**Total Modifications:** 6 files, ~255 lines changed

---

**Net Impact:**

| Metric              | Before  | After   | Change                                       |
| ------------------- | ------- | ------- | -------------------------------------------- |
| **Total Files**     | 155     | 151     | -4 files (-3%)                               |
| **Total Lines**     | ~25,000 | ~18,000 | -7,000 (-28%)                                |
| **Orchestration**   | 8,156   | 1,500   | -6,656 (-82%)                                |
| **New Directories** | 0       | 4       | +4 (workflows, activities, events, registry) |

**Key Insight:** Despite adding significant functionality (webhooks, MongoDB, Temporal), we're **reducing total codebase size by 28%** because we're eliminating 8,156 lines of custom orchestration code.

---

## 5. MongoDB Document Registry Integration

### 5.1 Document Schema Design

**MongoDB Collection: `documents`**

```javascript
{
  "_id": ObjectId("507f1f77bcf86cd799439011"),

  // Source Information
  "source": "google_drive",  // google_drive, github, s3, api_upload, email
  "source_id": "1abc123...",  // External ID from source system
  "tenant_id": "tenant_xyz",

  // File Metadata
  "file_name": "Q4_Financial_Report.pdf",
  "mime_type": "application/pdf",
  "size_bytes": 1048576,
  "hash": "sha256:abc123...",
  "storage_url": "s3://bucket/tenant_xyz/doc_abc123.pdf",

  // Classification & Sensitivity
  "classification": "CONFIDENTIAL",  // PUBLIC, INTERNAL, CONFIDENTIAL, RESTRICTED
  "sensitivity": "HIGH",  // LOW, MEDIUM, HIGH, CRITICAL
  "document_type": "FINANCIAL_REPORT",  // MEETING, CONTRACT, EMAIL, CODE, etc.
  "retention_policy": "7_YEARS",  // 30_DAYS, 1_YEAR, 7_YEARS, PERMANENT

  // ACL (Access Control List)
  "acl": {
    "owner": "john.doe@example.com",
    "allowed_users": ["jane.smith@example.com", "bob.jones@example.com"],
    "allowed_roles": ["FINANCE", "ADMIN", "EXECUTIVE"],
    "denied_users": ["contractor@example.com"],
    "public": false
  },

  // Lifecycle Status
  "lifecycle_status": "ACTIVE",  // PENDING, ACTIVE, ARCHIVED, DELETED

  // Indexing References (Cross-Database Links)
  "qdrant_point_ids": ["point_1", "point_2", "point_3"],  // Vector store references
  "neo4j_node_id": "node_abc123",  // Knowledge graph reference

  // Audit Trail
  "created_at": ISODate("2026-02-06T10:00:00Z"),
  "updated_at": ISODate("2026-02-06T10:30:00Z"),
  "indexed_at": ISODate("2026-02-06T10:30:00Z"),
  "created_by": "john.doe@example.com",
  "last_accessed_at": ISODate("2026-02-06T11:00:00Z"),
  "access_count": 42,

  // Workflow Tracking
  "workflow_id": "full-scan-tenant_xyz-20260206",
  "workflow_run_id": "abc123...",
  "indexing_status": "COMPLETED",  // PENDING, IN_PROGRESS, COMPLETED, FAILED
  "indexing_error": null
}
```

---

### 4.2 ACL Policy Management

**MongoDB as Policy Authority:**

```python
async def check_document_access(doc_id: str, user_email: str, user_roles: list[str]) -> bool:
    """
    Check if user has access to document.
    MongoDB is the single source of truth for ACL policies.
    """
    mongo_client = AsyncIOMotorClient(settings.mongodb_url)
    db = mongo_client.hai_indexer

    doc = await db.documents.find_one({"_id": ObjectId(doc_id)})
    if not doc:
        return False

    acl = doc.get("acl", {})

    # Check if public
    if acl.get("public", False):
        return True

    # Check if denied
    if user_email in acl.get("denied_users", []):
        return False

    # Check if owner
    if user_email == acl.get("owner"):
        return True

    # Check if in allowed users
    if user_email in acl.get("allowed_users", []):
        return True

    # Check if has allowed role
    allowed_roles = acl.get("allowed_roles", [])
    if any(role in allowed_roles for role in user_roles):
        return True

    return False
```

**Benefits:**

- âœ… **Single source of truth** - All ACL policies in MongoDB
- âœ… **Consistent enforcement** - Same logic across all services
- âœ… **Audit trail** - Track who accessed what when
- âœ… **Easy updates** - Change ACL in one place

---

### 4.3 Lifecycle Tracking

**Document Lifecycle States:**

```
PENDING â†’ ACTIVE â†’ ARCHIVED â†’ DELETED
   â†“         â†“         â†“
 FAILED   FAILED   FAILED
```

**Lifecycle Transitions:**

```python
async def transition_document_lifecycle(doc_id: str, new_status: str, reason: str):
    """Transition document lifecycle status."""
    mongo_client = AsyncIOMotorClient(settings.mongodb_url)
    db = mongo_client.hai_indexer

    await db.documents.update_one(
        {"_id": ObjectId(doc_id)},
        {
            "$set": {
                "lifecycle_status": new_status,
                "updated_at": datetime.utcnow(),
            },
            "$push": {
                "lifecycle_history": {
                    "status": new_status,
                    "reason": reason,
                    "timestamp": datetime.utcnow(),
                }
            }
        }
    )
```

**Use Cases:**

- âœ… **Retention policies** - Auto-archive documents after 7 years
- âœ… **Compliance** - Track document lifecycle for audits
- âœ… **Data cleanup** - Delete archived documents after retention period
- âœ… **Workflow integration** - Temporal workflows trigger lifecycle transitions

---

### 4.4 Integration with Indexing Pipeline

**Flow:**

```
1. Webhook â†’ Redis Streams â†’ Temporal Workflow
2. Temporal Activity: fetch_file_metadata()
3. Temporal Activity: register_in_mongodb() â† CREATE DOCUMENT RECORD
4. Temporal Activity: check_acl_policy() â† CHECK MONGODB ACL
5. Temporal Activity: download_file()
6. Temporal Activity: index_file() â†’ Qdrant
7. Temporal Activity: update_graph() â†’ Neo4j
8. Temporal Activity: update_mongodb_after_indexing() â† UPDATE WITH REFERENCES
```

**Key Integration Points:**

1. **Before Indexing:** Check MongoDB ACL to see if user has permission
2. **During Indexing:** Store qdrant_point_ids and neo4j_node_id in MongoDB
3. **After Indexing:** Update lifecycle_status to ACTIVE
4. **On Query:** Check MongoDB ACL before returning search results

---

### 4.5 Audit Trail

**Track All Document Access:**

```python
async def log_document_access(doc_id: str, user_email: str, action: str):
    """Log document access for audit trail."""
    mongo_client = AsyncIOMotorClient(settings.mongodb_url)
    db = mongo_client.hai_indexer

    await db.documents.update_one(
        {"_id": ObjectId(doc_id)},
        {
            "$set": {
                "last_accessed_at": datetime.utcnow(),
            },
            "$inc": {
                "access_count": 1,
            },
            "$push": {
                "access_log": {
                    "user": user_email,
                    "action": action,  # VIEW, DOWNLOAD, SHARE, DELETE
                    "timestamp": datetime.utcnow(),
                    "ip_address": request.client.host,
                }
            }
        }
    )
```

**Compliance Benefits:**

- âœ… **GDPR compliance** - Track who accessed personal data
- âœ… **SOC 2 compliance** - Audit trail for security reviews
- âœ… **HIPAA compliance** - Track access to medical records
- âœ… **Forensics** - Investigate security incidents

---

## 6. Complete System Architecture

### 6.1 System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXTERNAL SYSTEMS                                                            â”‚
â”‚  - Google Drive (webhooks)                                                   â”‚
â”‚  - GitHub (webhooks)                                                         â”‚
â”‚  - S3 (event notifications)                                                  â”‚
â”‚  - Email (IMAP)                                                              â”‚
â”‚  - MCP Tools                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HAI INDEXER APPLICATION                                                     â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ FastAPI (API Layer)                                                  â”‚   â”‚
â”‚  â”‚ - Webhook endpoints (/api/webhooks/*)                                â”‚   â”‚
â”‚  â”‚ - Index endpoints (/api/index/*)                                     â”‚   â”‚
â”‚  â”‚ - Search endpoints (/api/search/*)                                   â”‚   â”‚
â”‚  â”‚ - Workflow management (/api/workflows/*)                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Event Bus (Redis Streams)                                            â”‚   â”‚
â”‚  â”‚ - file-ingestion-events stream                                       â”‚   â”‚
â”‚  â”‚ - Consumer groups: temporal-workers, analytics-workers               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Workflow Orchestration (Temporal)                                    â”‚   â”‚
â”‚  â”‚ - FileIngestionWorkflow                                              â”‚   â”‚
â”‚  â”‚ - FullScanWorkflow                                                   â”‚   â”‚
â”‚  â”‚ - ERPSyncWorkflow                                                    â”‚   â”‚
â”‚  â”‚ - DocumentApprovalWorkflow                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Business Logic (Temporal Activities)                                 â”‚   â”‚
â”‚  â”‚ - File operations (fetch, download, list)                            â”‚   â”‚
â”‚  â”‚ - MongoDB operations (register, check ACL, update)                   â”‚   â”‚
â”‚  â”‚ - Indexing operations (index, update graph)                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Indexing Pipeline                                                    â”‚   â”‚
â”‚  â”‚ - Document classification                                            â”‚   â”‚
â”‚  â”‚ - Chunking & embedding generation                                    â”‚   â”‚
â”‚  â”‚ - Vector store upsert                                                â”‚   â”‚
â”‚  â”‚ - Knowledge graph building                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA LAYER                                                                  â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ MongoDB          â”‚  â”‚ Qdrant           â”‚  â”‚ Neo4j            â”‚          â”‚
â”‚  â”‚ (Doc Registry)   â”‚  â”‚ (Vectors)        â”‚  â”‚ (Graph)          â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚  â”‚ Redis            â”‚  â”‚ PostgreSQL       â”‚                                 â”‚
â”‚  â”‚ (Streams)        â”‚  â”‚ (Temporal State) â”‚                                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Migration Roadmap

### Phase 1: Infrastructure Setup (Week 1-2)

**Tasks:**

1. Add Temporal to docker-compose.yml
2. Add MongoDB to docker-compose.yml
3. Add PostgreSQL for Temporal state
4. Configure Redis Streams
5. Set up Temporal UI

**Deliverables:**

- âœ… All services running in docker-compose
- âœ… Temporal UI accessible at http://localhost:8080
- âœ… MongoDB accessible at mongodb://localhost:27017

---

### Phase 2: Parallel Operation (Week 3-4)

**Tasks:**

1. Create Temporal workflows (FileIngestionWorkflow, FullScanWorkflow)
2. Create Temporal activities
3. Create Temporal worker
4. Create Redis Streams consumer
5. Create webhook routes
6. Keep RQ running in parallel

**Deliverables:**

- âœ… Temporal workflows operational
- âœ… Webhooks functional
- âœ… RQ still handling existing jobs
- âœ… Both systems running side-by-side

---

### Phase 3: Migration (Week 5)

**Tasks:**

1. Migrate API routes from RQ to Temporal
2. Test all workflows
3. Monitor for issues
4. Gradual traffic shift (10% â†’ 50% â†’ 100%)

**Deliverables:**

- âœ… All API routes using Temporal
- âœ… RQ deprecated but still available
- âœ… 100% traffic on Temporal

---

### Phase 4: Cutover (Week 6)

**Tasks:**

1. Delete RQ worker files
2. Remove RQ from requirements.txt
3. Remove RQ from docker-compose.yml
4. Clean up old code

**Deliverables:**

- âœ… RQ completely removed
- âœ… Codebase cleaned up
- âœ… Documentation updated

---

## 8. Operational Considerations

### 8.1 Monitoring & Alerting

**Temporal UI Monitoring:**

- Workflow success/failure rates
- Activity retry counts
- Workflow duration metrics
- Queue depth

**MongoDB Monitoring:**

- Document count
- ACL policy violations
- Lifecycle transitions
- Access patterns

**Redis Streams Monitoring:**

- Stream length
- Consumer lag
- Event processing rate
- Failed events

---

## 9. Risk Assessment & Mitigation

### 9.1 Technical Risks

| Risk                     | Probability | Impact | Mitigation                          |
| ------------------------ | ----------- | ------ | ----------------------------------- |
| Temporal learning curve  | High        | Medium | 1-2 week training period            |
| MongoDB schema changes   | Medium      | Low    | Use flexible schema design          |
| Redis Streams complexity | Low         | Low    | Well-documented, production-proven  |
| Migration bugs           | Medium      | Medium | Parallel operation during migration |

---

## 10. Cost-Benefit Analysis

### 10.1 Development Costs

- **Engineering time:** 4-6 weeks (1-2 engineers)
- **Training:** 1-2 weeks
- **Testing:** 1 week

**Total:** ~8 weeks

### 10.2 Benefits

- **70% code reduction:** 8,156 â†’ ~1,500 lines
- **Maintenance savings:** ~50% less time debugging orchestration
- **Real-time ingestion:** 10x faster document indexing
- **Crash recovery:** 99.9% reliability vs 95% today

**ROI:** Positive within 6 months

---

## 11. Decision Framework

### 11.1 Go/No-Go Criteria

**GO if:**

- âœ… Need real-time webhooks
- âœ… Need crash recovery
- âœ… Need unlimited job duration
- âœ… Need document governance
- âœ… Want to reduce technical debt

**NO-GO if:**

- âŒ Current RQ solution is sufficient
- âŒ No bandwidth for 6-week migration
- âŒ No need for webhooks or real-time ingestion

---

## Conclusion

This architecture proposal provides a comprehensive upgrade path for HAI Indexer, addressing current limitations while positioning the system for future growth. The combination of **Temporal** (workflow orchestration), **Redis Streams** (event streaming), and **MongoDB** (document registry) creates a robust, scalable, and maintainable architecture.

**Recommendation:** **PROCEED** with phased migration starting with infrastructure setup.

---

**Document Status:** âœ… Complete and Ready for Review

**Next Steps:**

1. Review this document with stakeholders
2. Get approval for Phase 1 (Infrastructure Setup)
3. Begin implementation

**Questions?** Contact the architecture team.
